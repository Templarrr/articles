# AGI extinction estimates

WIP

There's a lot of people running around making huge calls on the dangers of AI development and how it is an existential level risk.
So I will do my own prediction and we'll see who is right.

First, few definitions. 

As defined in Wiki, an **extinction event** is a widespread and rapid decrease in the biodiversity on Earth. If we're talking about humanity extinction event than I'll define it as something that checks one of the following...

1. Directly or indirectly (within 2 levels of separation) caused the loss of 50% of human population within a single human lifespan (~70 years) with the causes of the event being understood afterwards and countermeasures for future occurrences available.
2. Directly or indirectly caused caused the loss of 10% of human population within a single human lifespan and for one or another reason it will be impossible to prevent the future repeats of the event.

Why not "total extinction"? Because whoever thinks anything smaller than cosmic-level event (gamma ray burst, huge asteroid impact etc) can cause it should either check with their dealer or simply don't understand how
big a task it is to lower population of 8 billion people under the minimum viable threshold. Even the full nuclear Armageddon with all countries using all the armory wouldn't do that (though survivors of that would probably wish...). 
The event as I described it would already be the biggest catastrophe humanity ever faced and drastically reduce the quality of life for centuries.

Why "within 2 levels of separation"? Because without a threshold there lies madness. You can't blame Hitler's grand-grand-grand-grand-parents for the Holocaust even though if any of them had chosen other partner or skipped
the occasion there wouldn't be one. We need to draw a line somewhere. So AI->deaths, AI->X->deaths, AI->X->Y->deaths counts, AI->X->Y->Z->deaths not. For me X,Y,Z would be the causes much more than an AI.

Within the given definition **I give an AGI an estimate of 5% of causing this in the next 100 years**. And don't get me wrong - it is a lot. It is more than the COVID mortality! That said, 4.9% of it I'll assign to the
people operators using the system outside the parameters it was supposed to be used. ChatGPT craziness demonstrated extremely clearly how people just don't understand the basic limitations of tools they operate.

So why would I think the AI doomers not just wrong, but dangerously wrong and causing a lot of harm with best intentions? Because **I give the humanity without an AGI an estimate of 50% of causing this in the next 100 years**.
We don't live in one point in time. When we compare of consequences of some action - we shouldn't compare it with the current state, but with the future state where we didn't take it. And the world without an AGI
for me is actively worse and more dangerous.

Mykhailo Odintsov

Principal Software Engineer in DataRobot
